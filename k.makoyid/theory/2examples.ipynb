{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO+DZ+2w1Zmdjk11I2Xvfaz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","\n","# Покроковий розрахунок проходження даних через нейромережу\n","\n","## 1. Препроцесинг вхідних даних\n","\n","### Жовте зображення (RGB формат)\n","Початкові значення для жовтого кольору (255, 255, 0)(кожен рядок це один піксель):\n","\n","$first = \\begin{bmatrix}\n","255, 255, 0 \\\\  \n"," 255, 255, 0 \\\\  \n"," 255, 255, 0 \\\\   \n"," 255, 255, 0  \n","\\end{bmatrix}$\n","\n","### Нормалізація (ділення на 255)\n","$x_1 = [1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0]^T \\in \\mathbb{R}^{12}$\n","\n","## 2. Ініціалізація параметрів мережі\n","\n","Припустимо, що у нас:\n","- Прихований шар: 4 нейрони\n","- Вихідний шар: 2 нейрони (для бінарної класифікації)\n","\n","Ініціалізація He для $W^{[1]}$ (12×4):\n","$\\sigma = \\sqrt{\\frac{2}{12}} \\approx 0.408$\n","\n","$W^{[1]} = \\begin{bmatrix}\n","0.2 & -0.3 & 0.1 & 0.3 \\\\\n","-0.1 & 0.35 & -0.2 & 0.25 \\\\\n","0.15 & -0.1 & 0.3 & -0.2 \\\\\n","0.3 & 0.2 & -0.4 & 0.1 \\\\\n","-0.2 & 0.25 & 0.3 & -0.3 \\\\\n","0.1 & -0.35 & 0.2 & 0.15 \\\\\n","0.35 & 0.3 & -0.1 & 0.2 \\\\\n","-0.3 & 0.1 & 0.4 & -0.1 \\\\\n","0.2 & -0.2 & 0.3 & 0.25 \\\\\n","0.25 & 0.3 & -0.3 & 0.2 \\\\\n","-0.1 & 0.35 & 0.2 & -0.25 \\\\\n","0.3 & -0.25 & 0.1 & 0.3\n","\\end{bmatrix}$\n","\n","$b^{[1]} = [0, 0, 0, 0]^T$\n","\n","\n","$a^{[1]}_1 = \\begin{bmatrix}\n","0.4 \\\\\n","1.55 \\\\\n","0 \\\\\n","0.4\n","\\end{bmatrix}$\n","\n","\n","\n","### Ініціалізація He для $W^{[2]}$ (4×2):\n","$\\sigma = \\sqrt{\\frac{2}{4}} = 0.707$\n","\n","$W^{[2]} = \\begin{bmatrix}\n","0.3 & -0.4 \\\\\n","-0.2 & 0.5 \\\\\n","0.4 & -0.3 \\\\\n","-0.5 & 0.2\n","\\end{bmatrix}$\n","\n","$b^{[2]} = [0, 0]^T$\n","### Вихідний шар\n","\n","1. Лінійна трансформація $z^{[2]}_1 = W^{[2]}a^{[1]}_1 + b^{[2]}$:\n","\n","$z^{[2]}_1 = \\begin{bmatrix}\n","0.3(0.4) + (-0.2)(1.55) + 0.4(0) + (-0.5)(0.4) \\\\\n","(-0.4)(0.4) + 0.5(1.55) + (-0.3)(0) + 0.2(0.4)\n","\\end{bmatrix} = \\begin{bmatrix}\n","-0.39 \\\\\n","0.695\n","\\end{bmatrix}$\n","\n","3. Forward propagation для жовтого зображення\n","\n","### Прихований шар\n","\n","$z^{[1]}_1 = W^{[1]}x_1 + b^{[1]} =$\n","$\\begin{bmatrix}\n","0.2 & -0.3 & 0.1 & 0.3 \\\\\n","-0.1 & 0.35 & -0.2 & 0.25 \\\\\n","0.15 & -0.1 & 0.3 & -0.2 \\\\\n","0.3 & 0.2 & -0.4 & 0.1 \\\\\n","-0.2 & 0.25 & 0.3 & -0.3 \\\\\n","0.1 & -0.35 & 0.2 & 0.15 \\\\\n","0.35 & 0.3 & -0.1 & 0.2 \\\\\n","-0.3 & 0.1 & 0.4 & -0.1 \\\\\n","0.2 & -0.2 & 0.3 & 0.25 \\\\\n","0.25 & 0.3 & -0.3 & 0.2 \\\\\n","-0.1 & 0.35 & 0.2 & -0.25 \\\\\n","0.3 & -0.25 & 0.1 & 0.3\n","\\end{bmatrix} \\begin{bmatrix}\n","1 \\\\\n","1 \\\\\n","0 \\\\\n","1 \\\\\n","1 \\\\\n","0 \\\\\n","1 \\\\\n","1 \\\\\n","0 \\\\\n","1 \\\\\n","1 \\\\\n","0\n","\\end{bmatrix} + \\begin{bmatrix}\n","0 \\\\\n","0 \\\\\n","0 \\\\\n","0\n","\\end{bmatrix}$\n","Рахуємо покомпонентно:\n","Перший елемент:\n","$0.2(1) + (-0.1)(1) + 0.15(0) + 0.3(1) + (-0.2)(1) + 0.1(0) + 0.35(1) + (-0.3)(1) + 0.2(0) + 0.25(1) + (-0.1)(1) + 0.3(0) =\\\\\n","0.2 - 0.1 + 0.3 - 0.2 + 0.35 - 0.3 + 0.25 - 0.1 = 0.4$\n","Другий елемент:\n","$(-0.3)(1) + 0.35(1) + (-0.1)(0) + 0.2(1) + 0.25(1) + (-0.35)(0) + 0.3(1) + 0.1(1) + (-0.2)(0) + 0.3(1) + 0.35(1) + (-0.25)(0) = -0.3 + 0.35 + 0.2 + 0.25 + 0.3 + 0.1 + 0.3 + 0.35 = 1.55$\n","Третій елемент:\n","$0.1(1) + (-0.2)(1) + 0.3(0) + (-0.4)(1) + 0.3(1) + 0.2(0) + (-0.1)(1) + 0.4(1) + 0.3(0) + (-0.3)(1) + 0.2(1) + 0.1(0) = 0.1 - 0.2 - 0.4 + 0.3 - 0.1 + 0.4 - 0.3 + 0.2 = 0$\n","Четвертий елемент:\n","$0.3(1) + 0.25(1) + (-0.2)(0) + 0.1(1) + (-0.3)(1) + 0.15(0) + 0.2(1) + (-0.1)(1) + 0.25(0) + 0.2(1) + (-0.25)(1) + 0.3(0) = 0.3 + 0.25 + 0.1 - 0.3 + 0.2 - 0.1 + 0.2 - 0.25 = 0.4$\n","Таким чином:\n","$z^{[1]}_1 = \\begin{bmatrix}\n","0.4 \\\\\n","1.55 \\\\\n","0 \\\\\n","0.4\n","\\end{bmatrix}$\n","2. ReLU активація $a^{[1]}_1 = \\max(0, z^{[1]}_1)$:\n","\n","$a^{[1]}_1 = \\begin{bmatrix}\n","0.4 \\\\\n","1.55 \\\\\n","0 \\\\\n","0.4\n","\\end{bmatrix}$\n","\n","\n","2. Softmax активація:\n","\n","$a^{[2]}_1 = \\text{softmax}(z^{[2]}_1) = \\begin{bmatrix}\n","\\frac{e^{-0.39}}{e^{-0.39} + e^{0.695}} \\\\\n","\\frac{e^{0.695}}{e^{-0.39} + e^{0.695}}\n","\\end{bmatrix} = \\begin{bmatrix}\n","0.25 \\\\\n","0.74\n","\\end{bmatrix}$\n","\n","## 4. Розрахунок loss\n","\n","Припустимо, що правильна мітка для жовтого зображення $y_1 = [1, 0]^T$\n","$y_2 = [0, 1]^T$\n","Cross-entropy loss:\n","$L_1 = -[1 \\cdot \\ln(0.25) + 0 \\cdot \\ln(0.74)] = 1.38$\n","\n","## 5. Backward propagation\n","\n","Градієнти вихідного шару:\n","\n","# Детальний розрахунок градієнтів (Backward Propagation)\n","\n","## 1. Градієнти вихідного шару\n","\n","### 1.1 Градієнт loss функції по відношенню до виходу softmax $\\frac{\\partial L_1}{\\partial z^{[2]}_1}$\n","$\\frac{\\partial L_1}{\\partial z^{[2]}_1} = a^{[2]}_1 - y_1$\n","\n","$= \\begin{bmatrix} 0.25 \\\\ 0.74 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} -0.75 \\\\ 0.74 \\end{bmatrix}$\n","\n","### 1.2 Градієнт по вагам другого шару $\\frac{\\partial L_1}{\\partial W^{[2]}}$\n","$\\frac{\\partial L_1}{\\partial W^{[2]}} = \\frac{\\partial L_1}{\\partial z^{[2]}_1}(a^{[1]}_1)^T$\n","\n","$= \\begin{bmatrix} -0.75 \\\\ 0.74 \\end{bmatrix} \\begin{bmatrix} 0.4 & 1.55 & 0 & 0.4 \\end{bmatrix}$\n","\n","$= \\begin{bmatrix}\n","(-0.75)(0.4) & (-0.75)(1.55) & (-0.75)(0) & (-0.75)(0.4) \\\\\n","(0.74)(0.4) & (0.74)(1.55) & (0.74)(0) & (0.74)(0.4)\n","\\end{bmatrix}$\n","\n","$= \\begin{bmatrix}\n","-0.3 & -1.16 & 0 & -0.3 \\\\\n","0.296 & 1.147 & 0 & 0.296\n","\\end{bmatrix}$\n","\n","### 1.3 Градієнт по зміщенням другого шару $\\frac{\\partial L_1}{\\partial b^{[2]}}$\n","$\\frac{\\partial L_1}{\\partial b^{[2]}} = \\frac{\\partial L_1}{\\partial z^{[2]}_1} = \\begin{bmatrix} -0.75 \\\\ 0.74 \\end{bmatrix}$\n","\n","## 2. Градієнти прихованого шару\n","\n","### 2.1 Градієнт по виходу прихованого шару $\\frac{\\partial L_1}{\\partial a^{[1]}_1}$\n","$\\frac{\\partial L_1}{\\partial a^{[1]}_1} = (W^{[2]})^T\\frac{\\partial L_1}{\\partial z^{[2]}_1}$\n","\n","$= \\begin{bmatrix}\n","0.3 & -0.4 \\\\\n","-0.2 & 0.5 \\\\\n","0.4 & -0.3 \\\\\n","-0.5 & 0.2\n","\\end{bmatrix}^T \\begin{bmatrix} -0.75 \\\\ 0.74 \\end{bmatrix}$\n","\n","$= \\begin{bmatrix}\n","0.3(-0.75) + (-0.4)(0.74) \\\\\n","(-0.2)(-0.75) + 0.5(0.74) \\\\\n","0.4(-0.75) + (-0.3)(0.74) \\\\\n","(-0.5)(-0.75) + 0.2(0.74)\n","\\end{bmatrix}$\n","\n","$= \\begin{bmatrix}\n","-0.225 - 0.296 \\\\\n","0.15 + 0.37 \\\\\n","-0.3 - 0.222 \\\\\n","0.375 + 0.148\n","\\end{bmatrix} = \\begin{bmatrix}\n","-0.521 \\\\\n","0.52 \\\\\n","-0.522 \\\\\n","0.523\n","\\end{bmatrix}$\n","\n","### 2.2 Градієнт по входу прихованого шару $\\frac{\\partial L_1}{\\partial z^{[1]}_1}$\n","$\\frac{\\partial L_1}{\\partial z^{[1]}_1} = \\frac{\\partial L_1}{\\partial a^{[1]}_1} \\odot \\mathbb{1}_{z^{[1]}_1 > 0}$\n","\n","Оскільки $z^{[1]}_1 = \\begin{bmatrix} 0.4 \\\\ 1.55 \\\\ 0 \\\\ 0.4 \\end{bmatrix}$, маска ReLU буде:\n","$\\mathbb{1}_{z^{[1]}_1 > 0} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix}$\n","\n","$\\frac{\\partial L_1}{\\partial z^{[1]}_1} = \\begin{bmatrix} -0.521 \\\\ 0.52 \\\\ 0 \\\\ 0.523 \\end{bmatrix}$\n","\n","### 2.3 Градієнт по вагам першого шару $\\frac{\\partial L_1}{\\partial W^{[1]}}$\n","$\\frac{\\partial L_1}{\\partial W^{[1]}} = \\frac{\\partial L_1}{\\partial z^{[1]}_1}x_1^T$\n","\n","$= \\begin{bmatrix} -0.521 \\\\ 0.52 \\\\ 0 \\\\ 0.523 \\end{bmatrix} \\begin{bmatrix} 1 & 1 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 1 & 1 & 0 \\end{bmatrix}$\n","\n","Це дасть матрицю 4×12, де кожен рядок - це відповідний елемент градієнта помножений на $x_1^T$\n","\n","### 2.4 Градієнт по зміщенням першого шару $\\frac{\\partial L_1}{\\partial b^{[1]}}$\n","$\\frac{\\partial L_1}{\\partial b^{[1]}} = \\frac{\\partial L_1}{\\partial z^{[1]}_1} = \\begin{bmatrix} -0.521 \\\\ 0.52 \\\\ 0 \\\\ 0.523 \\end{bmatrix}$\n","\n","## 3. Оновлення параметрів (з momentum)\n","\n","При $\\beta = 0.9$ та learning rate $\\alpha = 0.01$:\n","\n","### 3.1 Оновлення швидкості:\n","$v_{W^{[2]}} = 0.9v_{W^{[2]}} + 0.1\\begin{bmatrix}\n","-0.3 & -1.16 & 0 & -0.3 \\\\\n","0.296 & 1.147 & 0 & 0.296\n","\\end{bmatrix}$\n","\n","$v_{W^{[1]}} = 0.9v_{W^{[1]}} + 0.1(\\frac{\\partial L_1}{\\partial W^{[1]}})$\n","\n","$v_{b^{[2]}} = 0.9v_{b^{[2]}} + 0.1\\begin{bmatrix} -0.75 \\\\ 0.74 \\end{bmatrix}$\n","\n","$v_{b^{[1]}} = 0.9v_{b^{[1]}} + 0.1\\begin{bmatrix} -0.521 \\\\ 0.52 \\\\ 0 \\\\ 0.523 \\end{bmatrix}$\n","\n","### 3.2 Оновлення параметрів:\n","$W^{[2]}_{\\text{new}} = W^{[2]} - 0.01v_{W^{[2]}}$\n","$W^{[1]}_{\\text{new}} = W^{[1]} - 0.01v_{W^{[1]}}$\n","$b^{[2]}_{\\text{new}} = b^{[2]} - 0.01v_{b^{[2]}}$\n","$b^{[1]}_{\\text{new}} = b^{[1]} - 0.01v_{b^{[1]}}$\n","---------------------------------\n","\n","1) Навіщо потрібен градієнт:\n","- Градієнт показує напрямок найшвидшого зростання функції втрат (loss function)\n","- У контексті навчання нейромережі нам потрібно мінімізувати функцію втрат, тому ми рухаємося в протилежному напрямку градієнта\n","- Градієнт дозволяє зрозуміти, як саме треба змінити кожен параметр мережі (ваги та зміщення), щоб зменшити помилку\n","- По суті, градієнт - це \"компас\", який вказує напрямок для оптимізації параметрів мережі\n","\n","2) Чому градієнт представлений матрицями та векторами:\n","- У нейромережі параметри представлені у вигляді матриць (ваги) та векторів (зміщення)\n","- Згідно з правилами диференціювання, похідна по матриці/вектору дає результат такої ж розмірності\n","- Наприклад:\n","  - $W^{[1]}$ має розмір 12×4, тому $\\frac{\\partial L}{\\partial W^{[1]}}$ теж має розмір 12×4\n","  - $b^{[1]}$ має розмір 4×1, тому $\\frac{\\partial L}{\\partial b^{[1]}}$ теж має розмір 4×1\n","- Кожен елемент градієнтної матриці/вектора показує, як зміна відповідного параметра вплине на функцію втрат\n","\n","3) SGD + Momentum:\n","Формула оновлення параметрів з momentum:\n","\n","$v_t = \\beta v_{t-1} + (1-\\beta)\\nabla_\\theta L(\\theta_{t-1})$\n","$\\theta_t = \\theta_{t-1} - \\alpha v_t$\n","\n","де:\n","- $\\theta$ - будь-який параметр мережі ($W^{[1]}, W^{[2]}, b^{[1]}, b^{[2]}$)\n","- $v_t$ - накопичений momentum на кроці t\n","- $\\beta$ - коефіцієнт momentum (зазвичай 0.9)\n","- $\\alpha$ - швидкість навчання (learning rate)\n","- $\\nabla_\\theta L$ - градієнт функції втрат по параметру\n","\n","У контексті нашої мережі це виглядає так:\n","\n","Для ваг другого шару:\n","\n","$v_{W^{[2]}} = 0.9v_{W^{[2]}} + 0.1\\frac{\\partial L}{\\partial W^{[2]}}\n","W^{[2]} = W^{[2]} - 0.01v_{W^{[2]}}$$\n","\n","Momentum додає \"інерцію\" до процесу оптимізації:\n","- Накопичує інформацію про попередні напрямки градієнта\n","- Допомагає долати локальні мінімуми\n","- Згладжує коливання градієнта\n","- Прискорює навчання у напрямках, де градієнт стабільно вказує в один бік\n","# SGD + Momentum\n","\n","## Загальна формула:\n","$v_t = \\beta v_{t-1} + (1-\\beta)\\nabla_\\theta L(\\theta)$\n","$\\theta_t = \\theta_{t-1} - \\alpha v_t$\n","\n","де $\\beta = 0.9$, $\\alpha = 0.01$\n","\n","## Розрахунки для W²:\n","\n","$v_{W^{[2]}} = 0.9v_{W^{[2]}} + 0.1\\begin{bmatrix}\n","-0.3 & -1.16 & 0 & -0.3 \\\\\n","0.296 & 1.147 & 0 & 0.296\n","\\end{bmatrix}$\n","\n","Для елемента [0,0]:\n","$v_{W^{[2]}}[0,0] = 0.9 \\cdot 0 + 0.1 \\cdot (-0.3) = -0.03$\n","\n","$W^{[2]}_{new}[0,0] = 0.3 - 0.01 \\cdot (-0.03) = 0.3003$\n","\n","## Розрахунки для b²:\n","\n","$v_{b^{[2]}} = 0.9v_{b^{[2]}} + 0.1\\begin{bmatrix} -0.75 \\\\ 0.74 \\end{bmatrix}$\n","\n","$v_{b^{[2]}}[0] = 0.9 \\cdot 0 + 0.1 \\cdot (-0.75) = -0.075$\n","\n","$b^{[2]}_{new}[0] = 0 - 0.01 \\cdot (-0.075) = 0.00075$\n","\n","## Розрахунки для W¹:\n","\n","$v_{W^{[1]}} = 0.9v_{W^{[1]}} + 0.1(\\frac{\\partial L}{\\partial W^{[1]}})$\n","\n","де $\\frac{\\partial L}{\\partial W^{[1]}} = \\frac{\\partial L}{\\partial z^{[1]}}x_1^T$\n","\n","$W^{[1]}_{new} = W^{[1]} - 0.01v_{W^{[1]}}$\n","\n","## Розрахунки для b¹:\n","\n","$v_{b^{[1]}} = 0.9v_{b^{[1]}} + 0.1\\begin{bmatrix} -0.521 \\\\ 0.52 \\\\ 0 \\\\ 0.523 \\end{bmatrix}$\n","\n","$b^{[1]}_{new} = b^{[1]} - 0.01v_{b^{[1]}}$\n","\n","## Повне оновлення для всіх параметрів:\n","\n","$v_{W^{[2]}} = 0.9v_{W^{[2]}} + 0.1\\begin{bmatrix}\n","-0.3 & -1.16 & 0 & -0.3 \\\\\n","0.296 & 1.147 & 0 & 0.296\n","\\end{bmatrix}$\n","\n","$v_{b^{[2]}} = 0.9v_{b^{[2]}} + 0.1\\begin{bmatrix} -0.75 \\\\ 0.74 \\end{bmatrix}$\n","\n","$v_{W^{[1]}} = 0.9v_{W^{[1]}} + 0.1(\\frac{\\partial L}{\\partial z^{[1]}}x_1^T)$\n","\n","$v_{b^{[1]}} = 0.9v_{b^{[1]}} + 0.1\\begin{bmatrix} -0.521 \\\\ 0.52 \\\\ 0 \\\\ 0.523 \\end{bmatrix}$\n","\n","$W^{[2]}_{new} = W^{[2]} - 0.01v_{W^{[2]}}$\n","$b^{[2]}_{new} = b^{[2]} - 0.01v_{b^{[2]}}$\n","$W^{[1]}_{new} = W^{[1]} - 0.01v_{W^{[1]}}$\n","$b^{[1]}_{new} = b^{[1]} - 0.01v_{b^{[1]}}$\n","\n","Для випадку коли $v_{t-1} = 0$ (перша ітерація):\n","\n","$v_{W^{[2]}} = 0.1\\begin{bmatrix}\n","-0.3 & -1.16 & 0 & -0.3 \\\\\n","0.296 & 1.147 & 0 & 0.296\n","\\end{bmatrix} = \\begin{bmatrix}\n","-0.03 & -0.116 & 0 & -0.03 \\\\\n","0.0296 & 0.1147 & 0 & 0.0296\n","\\end{bmatrix}$\n","\n","$W^{[2]}_{new} = \\begin{bmatrix}\n","0.3 & -0.4 \\\\\n","-0.2 & 0.5 \\\\\n","0.4 & -0.3 \\\\\n","-0.5 & 0.2\n","\\end{bmatrix} - 0.01\\begin{bmatrix}\n","-0.03 & -0.116 & 0 & -0.03 \\\\\n","0.0296 & 0.1147 & 0 & 0.0296\n","\\end{bmatrix}$\n","\n","\n","--------------------------------------------------------------------------\n","Розрахунок функцій втрат\n","\n","## 1. Cross-Entropy Loss\n","\n","### Формула:\n","$L_{CE} = -\\sum_{i=1}^{n} y_i \\ln(a_i)$\n","\n","### Розрахунок:\n","$L_{CE} = -(1 \\cdot \\ln(0.25) + 0 \\cdot \\ln(0.74))$\n","\n","$= -(\\ln(0.25) + 0)$\n","\n","$= -(\\ln(\\frac{1}{4}))$\n","\n","$= -(-\\ln(4))$\n","\n","$= \\ln(4)$\n","\n","$\\approx 1.386$\n","\n","### Пояснення:\n","- Використовуємо лише перший доданок, бо $y_2 = 0$\n","- Більше значення loss показує, що модель дуже невпевнена у правильній відповіді\n","- Максимальне значення CE прямує до нескінченності\n","- Мінімальне значення CE = 0 (коли передбачення точне)\n","\n","## 2. Mean Squared Error (MSE)\n","\n","### Формула:\n","$L_{MSE} = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - a_i)^2$\n","\n","де n = 2 (розмірність вихідного вектора)\n","\n","### Розрахунок:\n","$L_{MSE} = \\frac{1}{2}[(1 - 0.25)^2 + (0 - 0.74)^2]$\n","\n","$= \\frac{1}{2}[(0.75)^2 + (0.74)^2]$\n","\n","$= \\frac{1}{2}[0.5625 + 0.5476]$\n","\n","$= \\frac{1.1101}{2}$\n","\n","$\\approx 0.55505$\n","\n","### Пояснення:\n","- MSE дає більшу вагу великим помилкам через квадрат\n","- Значення MSE завжди ≥ 0\n","- Ідеальне значення MSE = 0\n","- У нашому випадку помилки для обох класів приблизно однакові (0.75 та 0.74)\n","\n","## Порівняння:\n","1. CE краще підходить для класифікації, бо:\n","   - Сильніше штрафує за впевнені неправильні передбачення\n","   - Працює безпосередньо з ймовірностями\n","   - Враховує, що сума ймовірностей = 1\n","\n","2. MSE в даному випадку:\n","   - Дає менше абсолютне значення loss\n","   - Однаково оцінює помилки в обох напрямках\n","   - Може використовуватись для класифікації, але не оптимальний вибір\n","\n","## Градієнти для обох функцій:\n","\n","### Cross-Entropy:\n","$\\frac{\\partial L_{CE}}{\\partial a_i} = -\\frac{y_i}{a_i}$\n","\n","### MSE:\n","$\\frac{\\partial L_{MSE}}{\\partial a_i} = (a_i - y_i)$\n","\n","Це пояснює, чому CE краще для класифікації - градієнт більший для неправильних передбачень з високою впевненістю.\n","------------------------------------------------------------------------------------------------------\n","\n","\n","\\section{Детальні обчислення Backpropagation}\n","\n","\\subsection{Початкові значення (з Forward Pass)}\n","\n","Вхідні дані:\n","\\begin{align*}\n","x_1 &= \\begin{bmatrix}\n","1 & 1 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 1 & 1 & 0\n","\\end{bmatrix}^T \\\\\n","y_1 &= \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\n","\\end{align*}\n","\n","Ваги першого шару (ініціалізація He):\n","\\begin{equation*}\n","W^{[1]} = \\begin{bmatrix}\n","0.2 & -0.3 & 0.1 & 0.3 \\\\\n","-0.1 & 0.35 & -0.2 & 0.25 \\\\\n","0.15 & -0.1 & 0.3 & -0.2 \\\\\n","0.3 & 0.2 & -0.4 & 0.1 \\\\\n","-0.2 & 0.25 & 0.3 & -0.3 \\\\\n","0.1 & -0.35 & 0.2 & 0.15 \\\\\n","0.35 & 0.3 & -0.1 & 0.2 \\\\\n","-0.3 & 0.1 & 0.4 & -0.1 \\\\\n","0.2 & -0.2 & 0.3 & 0.25 \\\\\n","0.25 & 0.3 & -0.3 & 0.2 \\\\\n","-0.1 & 0.35 & 0.2 & -0.25 \\\\\n","0.3 & -0.25 & 0.1 & 0.3\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","Ваги другого шару:\n","\\begin{equation*}\n","W^{[2]} = \\begin{bmatrix}\n","0.3 & -0.4 \\\\\n","-0.2 & 0.5 \\\\\n","0.4 & -0.3 \\\\\n","-0.5 & 0.2\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","Зміщення:\n","\\begin{align*}\n","b^{[1]} &= \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\\\\n","b^{[2]} &= \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n","\\end{align*}\n","\n","\\subsection{Результати Forward Pass}\n","\n","Перший шар (до ReLU):\n","\\begin{equation*}\n","z^{[1]} = W^{[1]}x_1 + b^{[1]} = \\begin{bmatrix} 0.4 \\\\ 1.55 \\\\ 0 \\\\ 0.4 \\end{bmatrix}\n","\\end{equation*}\n","\n","Після ReLU:\n","\\begin{equation*}\n","a^{[1]} = \\text{ReLU}(z^{[1]}) = \\begin{bmatrix} 0.4 \\\\ 1.55 \\\\ 0 \\\\ 0.4 \\end{bmatrix}\n","\\end{equation*}\n","\n","Другий шар (до softmax):\n","\\begin{equation*}\n","z^{[2]} = W^{[2]}a^{[1]} + b^{[2]} = \\begin{bmatrix} -0.39 \\\\ 0.695 \\end{bmatrix}\n","\\end{equation*}\n","\n","Після softmax:\n","\\begin{equation*}\n","a^{[2]} = \\text{softmax}(z^{[2]}) = \\begin{bmatrix} 0.25 \\\\ 0.74 \\end{bmatrix}\n","\\end{equation*}\n","\n","\\subsection{Обчислення Loss}\n","\n","Cross-entropy loss:\n","\\begin{align*}\n","L &= -\\sum_{i=1}^{2} y_i \\ln(a^{[2]}_i) \\\\\n","&= -(1 \\cdot \\ln(0.25) + 0 \\cdot \\ln(0.74)) \\\\\n","&= -\\ln(0.25) \\\\\n","&= 1.386\n","\\end{align*}\n","\n","\\subsection{Backpropagation}\n","\n","\\subsubsection{Градієнт останнього шару}\n","\n","Градієнт softmax + cross-entropy:\n","\\begin{align*}\n","\\frac{\\partial L}{\\partial z^{[2]}} &= a^{[2]} - y_1 \\\\\n","&= \\begin{bmatrix} 0.25 \\\\ 0.74 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\\\\n","&= \\begin{bmatrix} -0.75 \\\\ 0.74 \\end{bmatrix}\n","\\end{align*}\n","\n","\\subsubsection{Градієнти для ваг другого шару}\n","\n","\\begin{align*}\n","\\frac{\\partial L}{\\partial W^{[2]}} &= \\frac{\\partial L}{\\partial z^{[2]}} (a^{[1]})^T \\\\\n","&= \\begin{bmatrix} -0.75 \\\\ 0.74 \\end{bmatrix} \\begin{bmatrix} 0.4 & 1.55 & 0 & 0.4 \\end{bmatrix} \\\\\n","&= \\begin{bmatrix}\n","(-0.75)(0.4) & (-0.75)(1.55) & (-0.75)(0) & (-0.75)(0.4) \\\\\n","(0.74)(0.4) & (0.74)(1.55) & (0.74)(0) & (0.74)(0.4)\n","\\end{bmatrix} \\\\\n","&= \\begin{bmatrix}\n","-0.3 & -1.16 & 0 & -0.3 \\\\\n","0.296 & 1.147 & 0 & 0.296\n","\\end{bmatrix}\n","\\end{align*}\n","\n","\\subsubsection{Градієнт для прихованого шару}\n","\n","\\begin{align*}\n","\\frac{\\partial L}{\\partial a^{[1]}} &= (W^{[2]})^T \\frac{\\partial L}{\\partial z^{[2]}} \\\\\n","&= \\begin{bmatrix}\n","0.3 & -0.4 \\\\\n","-0.2 & 0.5 \\\\\n","0.4 & -0.3 \\\\\n","-0.5 & 0.2\n","\\end{bmatrix}^T \\begin{bmatrix} -0.75 \\\\ 0.74 \\end{bmatrix} \\\\\n","&= \\begin{bmatrix}\n","(0.3)(-0.75) + (-0.4)(0.74) \\\\\n","(-0.2)(-0.75) + (0.5)(0.74) \\\\\n","(0.4)(-0.75) + (-0.3)(0.74) \\\\\n","(-0.5)(-0.75) + (0.2)(0.74)\n","\\end{bmatrix} \\\\\n","&= \\begin{bmatrix}\n","-0.225 - 0.296 \\\\\n","0.15 + 0.37 \\\\\n","-0.3 - 0.222 \\\\\n","0.375 + 0.148\n","\\end{bmatrix} \\\\\n","&= \\begin{bmatrix}\n","-0.521 \\\\\n","0.52 \\\\\n","-0.522 \\\\\n","0.523\n","\\end{bmatrix}\n","\\end{align*}\n","\n","\\subsubsection{Градієнт через ReLU}\n","\n","\\begin{align*}\n","\\frac{\\partial L}{\\partial z^{[1]}} &= \\frac{\\partial L}{\\partial a^{[1]}} \\odot \\mathbb{1}_{z^{[1]} > 0} \\\\\n","&= \\begin{bmatrix} -0.521 \\\\ 0.52 \\\\ -0.522 \\\\ 0.523 \\end{bmatrix} \\odot \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix} \\\\\n","&= \\begin{bmatrix} -0.521 \\\\ 0.52 \\\\ 0 \\\\ 0.523 \\end{bmatrix}\n","\\end{align*}\n","\n","\\subsubsection{Градієнти для ваг першого шару}\n","\n","\\begin{align*}\n","\\frac{\\partial L}{\\partial W^{[1]}} &= \\frac{\\partial L}{\\partial z^{[1]}} x_1^T \\\\\n","&= \\begin{bmatrix} -0.521 \\\\ 0.52 \\\\ 0 \\\\ 0.523 \\end{bmatrix} \\begin{bmatrix} 1 & 1 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 1 & 1 & 0 \\end{bmatrix}\n","\\end{align*}\n","\n","\\subsection{Оновлення параметрів (SGD + Momentum)}\n","\n","Для $\\beta = 0.9$ та $\\alpha = 0.01$:\n","\n","\\subsubsection{Ваги другого шару}\n","\n","Швидкість:\n","\\begin{align*}\n","v_{W^{[2]}} &= 0.9 \\cdot 0 + 0.1 \\cdot \\frac{\\partial L}{\\partial W^{[2]}} \\\\\n","&= 0.1 \\begin{bmatrix}\n","-0.3 & -1.16 & 0 & -0.3 \\\\\n","0.296 & 1.147 & 0 & 0.296\n","\\end{bmatrix} \\\\\n","&= \\begin{bmatrix}\n","-0.03 & -0.116 & 0 & -0.03 \\\\\n","0.0296 & 0.1147 & 0 & 0.0296\n","\\end{bmatrix}\n","\\end{align*}\n","\n","Оновлення:\n","\\begin{align*}\n","W^{[2]}_{\\text{new}} &= W^{[2]} - \\alpha v_{W^{[2]}} \\\\\n","&= \\begin{bmatrix}\n","0.3 & -0.4 \\\\\n","-0.2 & 0.5 \\\\\n","0.4 & -0.3 \\\\\n","-0.5 & 0.2\n","\\end{bmatrix} - 0.01 \\begin{bmatrix}\n","-0.03 & -0.116 & 0 & -0.03 \\\\\n","0.0296 & 0.1147 & 0 & 0.0296\n","\\end{bmatrix}^T\n","\\end{align*}\n","\n","\\subsubsection{Зміщення другого шару}\n","\n","\\begin{align*}\n","v_{b^{[2]}} &= 0.9 \\cdot 0 + 0.1 \\cdot \\begin{bmatrix} -0.75 \\\\ 0.74 \\end{bmatrix} \\\\\n","&= \\begin{bmatrix} -0.075 \\\\ 0.074 \\end{bmatrix}\n","\\end{align*}\n","\n","\\begin{align*}\n","b^{[2]}_{\\text{new}} &= b^{[2]} - \\alpha v_{b^{[2]}} \\\\\n","&= \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} - 0.01 \\begin{bmatrix} -0.075 \\\\ 0.074 \\end{bmatrix} \\\\\n","&= \\begin{bmatrix} 0.00075 \\\\ -0.00074 \\end{bmatrix}\n","\\end{align*}\n","\n","---------------------------------------------------------------------------------------------------------------------------------------------\n","\\section{Оновлені параметри мережі після проходження жовтого зображення}\n","\n","\\subsection{Оновлена матриця ваг першого шару $W^{[1]}_{\\text{new}}$}\n","\n","% Розрахунок градієнта для W^[1]\n","\\begin{equation*}\n","\\frac{\\partial L}{\\partial W^{[1]}} = \\begin{bmatrix}\n","-0.521 & 0.52 & 0 & 0.523\n","\\end{bmatrix} \\begin{bmatrix}\n","1 & 1 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 1 & 1 & 0\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","% Оновлена швидкість для W^[1]\n","\\begin{equation*}\n","v_{W^{[1]}} = 0.1 \\cdot \\frac{\\partial L}{\\partial W^{[1]}} = \\begin{bmatrix}\n","-0.0521 & -0.0521 & 0 & -0.0521 & -0.0521 & 0 & -0.0521 & -0.0521 & 0 & -0.0521 & -0.0521 & 0 \\\\\n","0.052 & 0.052 & 0 & 0.052 & 0.052 & 0 & 0.052 & 0.052 & 0 & 0.052 & 0.052 & 0 \\\\\n","0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n","0.0523 & 0.0523 & 0 & 0.0523 & 0.0523 & 0 & 0.0523 & 0.0523 & 0 & 0.0523 & 0.0523 & 0\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","% Оновлена матриця W^[1]\n","\\begin{equation*}\n","W^{[1]}_{\\text{new}} = \\begin{bmatrix}\n","0.2052 & -0.2948 & 0.1 & 0.3052 \\\\\n","-0.0948 & 0.3552 & -0.2 & 0.2552 \\\\\n","0.15 & -0.1 & 0.3 & -0.2 \\\\\n","0.3052 & 0.2052 & -0.4 & 0.1052 \\\\\n","-0.1948 & 0.2552 & 0.3 & -0.2948 \\\\\n","0.1 & -0.35 & 0.2 & 0.15 \\\\\n","0.3552 & 0.3052 & -0.1 & 0.2052 \\\\\n","-0.2948 & 0.1052 & 0.4 & -0.0948 \\\\\n","0.2 & -0.2 & 0.3 & 0.25 \\\\\n","0.2552 & 0.3052 & -0.3 & 0.2052 \\\\\n","-0.0948 & 0.3552 & 0.2 & -0.2448 \\\\\n","0.3 & -0.25 & 0.1 & 0.3\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","\\subsection{Оновлена матриця ваг другого шару $W^{[2]}_{\\text{new}}$}\n","\n","% Швидкість для W^[2]\n","\\begin{equation*}\n","v_{W^{[2]}} = \\begin{bmatrix}\n","-0.03 & -0.116 & 0 & -0.03 \\\\\n","0.0296 & 0.1147 & 0 & 0.0296\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","% Оновлена матриця W^[2]\n","\\begin{equation*}\n","W^{[2]}_{\\text{new}} = \\begin{bmatrix}\n","0.3003 & -0.3884 \\\\\n","-0.1988 & 0.5115 \\\\\n","0.4 & -0.3 \\\\\n","-0.4997 & 0.2003\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","\\subsection{Оновлені вектори зміщення}\n","\n","\\subsubsection{Перший шар $b^{[1]}_{\\text{new}}$}\n","\n","% Швидкість для b^[1]\n","\\begin{equation*}\n","v_{b^{[1]}} = 0.1 \\cdot \\begin{bmatrix}\n","-0.521 \\\\\n","0.52 \\\\\n","0 \\\\\n","0.523\n","\\end{bmatrix} = \\begin{bmatrix}\n","-0.0521 \\\\\n","0.052 \\\\\n","0 \\\\\n","0.0523\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","% Оновлений b^[1]\n","\\begin{equation*}\n","b^{[1]}_{\\text{new}} = \\begin{bmatrix}\n","0.00052 \\\\\n","-0.00052 \\\\\n","0 \\\\\n","-0.00052\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","\\subsubsection{Другий шар $b^{[2]}_{\\text{new}}$}\n","\n","% Швидкість для b^[2]\n","\\begin{equation*}\n","v_{b^{[2]}} = 0.1 \\cdot \\begin{bmatrix}\n","-0.75 \\\\\n","0.74\n","\\end{bmatrix} = \\begin{bmatrix}\n","-0.075 \\\\\n","0.074\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","% Оновлений b^[2]\n","\\begin{equation*}\n","b^{[2]}_{\\text{new}} = \\begin{bmatrix}\n","0.00075 \\\\\n","-0.00074\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","\n","\\section{Проходження зеленого зображення через оновлену нейрону мережу}\n","\n","## 1. Вхідні дані\n","\n","### 1.1 Вхідний вектор (зелене зображення 2×2)\n","\\begin{equation*}\n","x_{\\text{green}} = \\begin{bmatrix} 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\end{bmatrix}^T\n","\\end{equation*}\n","\n","### 1.2 Оновлені параметри мережі\n","Використовуємо оновлені параметри після проходження жовтого зображення:\n","- $W^{[1]}_{\\text{new}}$ - матриця ваг першого шару\n","- $b^{[1]}_{\\text{new}}$ - вектор зміщення першого шару\n","- $W^{[2]}_{\\text{new}}$ - матриця ваг другого шару\n","- $b^{[2]}_{\\text{new}}$ - вектор зміщення другого шару\n","\n","## 2. Forward Propagation\n","\n","### 2.1 Перший шар\n","\n","#### 2.1.1 Лінійна трансформація\n","\\begin{equation*}\n","z^{[1]} = W^{[1]}_{\\text{new}}x_{\\text{green}} + b^{[1]}_{\\text{new}}\n","\\end{equation*}\n","\n","\\begin{equation*}\n","z^{[1]} = \\begin{bmatrix}\n","-0.1844 \\\\\n","0.2604 \\\\\n","0.2 \\\\\n","0.1156\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","#### 2.1.2 ReLU активація\n","\\begin{equation*}\n","a^{[1]} = \\text{ReLU}(z^{[1]}) = \\max(0, z^{[1]})\n","\\end{equation*}\n","\n","\\begin{equation*}\n","a^{[1]} = \\begin{bmatrix}\n","0 \\\\\n","0.2604 \\\\\n","0.2 \\\\\n","0.1156\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","### 2.2 Другий шар\n","\n","#### 2.2.1 Лінійна трансформація\n","\\begin{equation*}\n","z^{[2]} = W^{[2]}_{\\text{new}}a^{[1]} + b^{[2]}_{\\text{new}}\n","\\end{equation*}\n","\n","\\begin{equation*}\n","z^{[2]} = \\begin{bmatrix}\n","-0.2276 \\\\\n","0.2224\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","#### 2.2.2 Softmax активація\n","\\begin{equation*}\n","a^{[2]} = \\text{softmax}(z^{[2]}) = \\frac{e^{z^{[2]}_i}}{\\sum_{j=1}^2 e^{z^{[2]}_j}}\n","\\end{equation*}\n","\n","\\begin{equation*}\n","a^{[2]} = \\begin{bmatrix}\n","0.3876 \\\\\n","0.6124\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","## 3. Loss Function\n","\n","### 3.1 Cross-Entropy Loss\n","Для зеленого зображення цільовий вектор:\n","\\begin{equation*}\n","y_{\\text{green}} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n","\\end{equation*}\n","\n","\\begin{equation*}\n","L = -\\sum_{i=1}^2 y_i \\log(a^{[2]}_i) = -\\log(0.6124) = 0.4902\n","\\end{equation*}\n","\n","## 4. Backward Propagation\n","\n","### 4.1 Градієнт по виходу другого шару\n","\\begin{equation*}\n","\\frac{\\partial L}{\\partial a^{[2]}} = \\begin{bmatrix}\n","0.3876 \\\\\n","-0.3876\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","### 4.2 Градієнти другого шару\n","\\begin{equation*}\n","\\frac{\\partial L}{\\partial W^{[2]}} = \\frac{\\partial L}{\\partial a^{[2]}} (a^{[1]})^T\n","\\end{equation*}\n","\n","\\begin{equation*}\n","\\frac{\\partial L}{\\partial b^{[2]}} = \\frac{\\partial L}{\\partial a^{[2]}}\n","\\end{equation*}\n","\n","### 4.3 Градієнти першого шару\n","\\begin{equation*}\n","\\frac{\\partial L}{\\partial a^{[1]}} = (W^{[2]}_{\\text{new}})^T \\frac{\\partial L}{\\partial a^{[2]}}\n","\\end{equation*}\n","\n","\\begin{equation*}\n","\\frac{\\partial L}{\\partial W^{[1]}} = \\frac{\\partial L}{\\partial a^{[1]}} \\text{diag}(\\text{ReLU}'(z^{[1]})) (x_{\\text{green}})^T\n","\\end{equation*}\n","\n","\\begin{equation*}\n","\\frac{\\partial L}{\\partial b^{[1]}} = \\frac{\\partial L}{\\partial a^{[1]}} \\text{diag}(\\text{ReLU}'(z^{[1]}))\n","\\end{equation*}\n","-------------------------------------------------------------------------------------\n","# Детальний розрахунок першого шару нейронної мережі\n","\n","## 1. Початкові дані\n","\n","### 1.1 Вхідний вектор (зелене зображення)\n","\\begin{equation*}\n","x_{\\text{green}} = \\begin{bmatrix} 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\end{bmatrix}^T\n","\\end{equation*}\n","\n","### 1.2 Оновлена матриця ваг першого шару (повна)\n","\\begin{equation*}\n","W^{[1]}_{\\text{new}} = \\begin{bmatrix}\n","0.2052 & -0.2948 & 0.1 & 0.3052 & -0.1948 & 0.1 & 0.3552 & -0.2948 & 0.2 & 0.2552 & -0.0948 & 0.3 \\\\\n","-0.0948 & 0.3552 & -0.2 & 0.2552 & 0.2552 & -0.35 & 0.3052 & 0.1052 & -0.2 & 0.3052 & 0.3552 & -0.25 \\\\\n","0.15 & -0.1 & 0.3 & -0.2 & 0.3 & 0.2 & -0.1 & 0.4 & 0.3 & -0.3 & 0.2 & 0.1 \\\\\n","0.3052 & 0.2052 & -0.4 & 0.1052 & -0.2948 & 0.15 & 0.2052 & -0.0948 & 0.25 & 0.2052 & -0.2448 & 0.3\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","### 1.3 Оновлений вектор зміщення\n","\\begin{equation*}\n","b^{[1]}_{\\text{new}} = \\begin{bmatrix} 0.00052 \\\\ -0.00052 \\\\ 0 \\\\ -0.00052 \\end{bmatrix}\n","\\end{equation*}\n","\n","## 2. Детальний розрахунок $z^{[1]}$\n","\n","### 2.1 Матричне множення $W^{[1]}_{\\text{new}}x_{\\text{green}}$\n","\n","Розпишемо по елементам:\n","\n","\\begin{equation*}\n","(W^{[1]}_{\\text{new}}x_{\\text{green}})_1 = (-0.2948 \\cdot 1) + (-0.1948 \\cdot 1) + (-0.2948 \\cdot 1) + (-0.0948 \\cdot 1) = -0.1844\n","\\end{equation*}\n","\n","\\begin{equation*}\n","(W^{[1]}_{\\text{new}}x_{\\text{green}})_2 = (0.3552 \\cdot 1) + (0.2552 \\cdot 1) + (0.1052 \\cdot 1) + (0.3552 \\cdot 1) = 0.2604\n","\\end{equation*}\n","\n","\\begin{equation*}\n","(W^{[1]}_{\\text{new}}x_{\\text{green}})_3 = (-0.1 \\cdot 1) + (0.3 \\cdot 1) + (0.4 \\cdot 1) + (0.2 \\cdot 1) = 0.2\n","\\end{equation*}\n","\n","\\begin{equation*}\n","(W^{[1]}_{\\text{new}}x_{\\text{green}})_4 = (0.2052 \\cdot 1) + (-0.2948 \\cdot 1) + (-0.0948 \\cdot 1) + (-0.2448 \\cdot 1) = 0.1156\n","\\end{equation*}\n","\n","### 2.2 Додавання зміщення (bias)\n","\n","\\begin{equation*}\n","z^{[1]} = W^{[1]}_{\\text{new}}x_{\\text{green}} + b^{[1]}_{\\text{new}} = \\begin{bmatrix} -0.1844 \\\\ 0.2604 \\\\ 0.2 \\\\ 0.1156 \\end{bmatrix} + \\begin{bmatrix} 0.00052 \\\\ -0.00052 \\\\ 0 \\\\ -0.00052 \\end{bmatrix} = \\begin{bmatrix} -0.18388 \\\\ 0.25988 \\\\ 0.2 \\\\ 0.11508 \\end{bmatrix}\n","\\end{equation*}\n","\n","## 3. Вплив швидкості навчання\n","\n","### 3.1 Формула оновлення ваг з урахуванням швидкості навчання\n","\\begin{equation*}\n","W^{[1]}_{\\text{new}} = W^{[1]}_{\\text{old}} - \\alpha \\cdot v_{W^{[1]}}\n","\\end{equation*}\n","\n","де:\n","- $\\alpha = 0.1$ - швидкість навчання\n","- $v_{W^{[1]}}$ - швидкість зміни ваг (momentum)\n","\n","### 3.2 Приклад впливу швидкості навчання\n","Для елемента $W^{[1]}_{11}$:\n","\\begin{equation*}\n","W^{[1]}_{11_{\\text{new}}} = W^{[1]}_{11_{\\text{old}}} - 0.1 \\cdot (-0.0521) = 0.2 + 0.00521 = 0.2052\n","\\end{equation*}\n","\n","Якби швидкість навчання була більшою, наприклад $\\alpha = 0.5$:\n","\\begin{equation*}\n","W^{[1]}_{11_{\\text{new}}} = 0.2 + 0.02605 = 0.22605\n","\\end{equation*}\n","\n","### 3.3 Вплив на зміщення\n","Аналогічно для bias:\n","\\begin{equation*}\n","b^{[1]}_{\\text{new}} = b^{[1]}_{\\text{old}} - \\alpha \\cdot v_{b^{[1]}}\n","\\end{equation*}\n","\n","Приклад для першого елемента:\n","\\begin{equation*}\n","b^{[1]}_{1_{\\text{new}}} = 0 - 0.1 \\cdot (-0.0521) = 0.00052\n","\\end{equation*}\n","\n","## 4. Пояснення впливу bias\n","\n","1. Біас діє як постійне зміщення для кожного нейрона незалежно від вхідних даних\n","2. Дозволяє нейрону \"активуватися\" навіть при нульових вхідних даних\n","3. В нашому випадку:\n","   - Для першого нейрона: -0.1844 + 0.00052 = -0.18388\n","   - Навіть малий біас може впливати на активацію ReLU, особливо коли значення близькі до 0\n","\n","   # Детальний розрахунок другого шару нейронної мережі та softmax\n","\n","## 1. Початкові дані\n","\n","### 1.1 Вхідний вектор (вихід з першого шару після ReLU)\n","\\begin{equation*}\n","a^{[1]} = \\begin{bmatrix}\n","0 \\\\\n","0.2604 \\\\\n","0.2 \\\\\n","0.1156\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","### 1.2 Оновлена матриця ваг другого шару\n","\\begin{equation*}\n","W^{[2]}_{\\text{new}} = \\begin{bmatrix}\n","0.3003 & -0.3884 & 0.4 & -0.4997 \\\\\n","-0.1988 & 0.5115 & -0.3 & 0.2003\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","### 1.3 Оновлений вектор зміщення другого шару\n","\\begin{equation*}\n","b^{[2]}_{\\text{new}} = \\begin{bmatrix}\n","0.00075 \\\\\n","-0.00074\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","## 2. Детальний розрахунок $z^{[2]}$\n","\n","### 2.1 Матричне множення $W^{[2]}_{\\text{new}}a^{[1]}$\n","\n","Розпишемо по елементам для першого нейрона:\n","\\begin{equation*}\n","\\begin{split}\n","(W^{[2]}_{\\text{new}}a^{[1]})_1 &= (0.3003 \\cdot 0) + (-0.3884 \\cdot 0.2604) + (0.4 \\cdot 0.2) + (-0.4997 \\cdot 0.1156) \\\\\n","&= 0 + (-0.1011) + 0.08 + (-0.0578) \\\\\n","&= -0.2284\n","\\end{split}\n","\\end{equation*}\n","\n","Для другого нейрона:\n","\\begin{equation*}\n","\\begin{split}\n","(W^{[2]}_{\\text{new}}a^{[1]})_2 &= (-0.1988 \\cdot 0) + (0.5115 \\cdot 0.2604) + (-0.3 \\cdot 0.2) + (0.2003 \\cdot 0.1156) \\\\\n","&= 0 + 0.1332 + (-0.06) + 0.0232 \\\\\n","&= 0.2231\n","\\end{split}\n","\\end{equation*}\n","\n","### 2.2 Додавання зміщення (bias)\n","\n","\\begin{equation*}\n","z^{[2]} = W^{[2]}_{\\text{new}}a^{[1]} + b^{[2]}_{\\text{new}} = \\begin{bmatrix}\n","-0.2284 \\\\\n","0.2231\n","\\end{bmatrix} + \\begin{bmatrix}\n","0.00075 \\\\\n","-0.00074\n","\\end{bmatrix} = \\begin{bmatrix}\n","-0.2276 \\\\\n","0.2224\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","## 3. Детальний розрахунок softmax\n","\n","### 3.1 Обчислення експонент\n","\\begin{equation*}\n","e^{z^{[2]}_1} = e^{-0.2276} = 0.7964\n","\\end{equation*}\n","\n","\\begin{equation*}\n","e^{z^{[2]}_2} = e^{0.2224} = 1.2490\n","\\end{equation*}\n","\n","### 3.2 Обчислення суми експонент\n","\\begin{equation*}\n","\\sum_{j=1}^2 e^{z^{[2]}_j} = 0.7964 + 1.2490 = 2.0454\n","\\end{equation*}\n","\n","### 3.3 Обчислення softmax ймовірностей\n","\\begin{equation*}\n","\\text{softmax}(z^{[2]}_1) = \\frac{e^{z^{[2]}_1}}{\\sum_{j=1}^2 e^{z^{[2]}_j}} = \\frac{0.7964}{2.0454} = 0.3876\n","\\end{equation*}\n","\n","\n","\n","\\begin{equation*}\n","\\text{softmax}(z^{[2]}_2) = \\frac{e^{z^{[2]}_2}}{\\sum_{j=1}^2 e^{z^{[2]}_j}} = \\frac{1.2490}{2.0454} = 0.6124\n","\\end{equation*}\n","\n","### 3.4 Фінальний вихід\n","\\begin{equation*}\n","a^{[2]} = \\begin{bmatrix}\n","0.3876 \\\\\n","0.6124\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","## 4. Інтерпретація результатів\n","\n","1. Перший вихід (0.3876) відповідає ймовірності належності до першого класу (жовтий)\n","2. Другий вихід (0.6124) відповідає ймовірності належності до другого класу (зелений)\n","3. Сума ймовірностей: 0.3876 + 0.6124 = 1\n","4. Мережа правильно класифікувала зелене зображення, надавши більшу ймовірність другому класу\n","\\end{document}\n","\n","\n","--------------------------------------------------------------\n","\n","\n","# Детальний розрахунок backpropagation для зеленого зображення\n","\n","## 1. Початкові дані\n","\n","### 1.1 Цільовий вектор для зеленого зображення\n","\\begin{equation*}\n","y = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n","\\end{equation*}\n","\n","### 1.2 Вихід мережі (після softmax)\n","\\begin{equation*}\n","a^{[2]} = \\begin{bmatrix} 0.3876 \\\\ 0.6124 \\end{bmatrix}\n","\\end{equation*}\n","\n","### 1.3 Активації першого шару (після ReLU)\n","\\begin{equation*}\n","a^{[1]} = \\begin{bmatrix} 0 \\\\ 0.2604 \\\\ 0.2 \\\\ 0.1156 \\end{bmatrix}\n","\\end{equation*}\n","\n","## 2. Градієнт Loss Function\n","\n","### 2.1 Cross-Entropy Loss\n","\\begin{equation*}\n","L = -\\sum_{i=1}^2 y_i \\log(a^{[2]}_i) = -\\log(0.6124) = 0.4902\n","\\end{equation*}\n","\n","### 2.2 Градієнт по виходу softmax\n","\\begin{equation*}\n","\\frac{\\partial L}{\\partial a^{[2]}} = \\begin{bmatrix}\n","\\frac{a^{[2]}_1 - y_1}{a^{[2]}_1} \\\\\n","\\frac{a^{[2]}_2 - y_2}{a^{[2]}_2}\n","\\end{bmatrix} = \\begin{bmatrix}\n","\\frac{0.3876 - 0}{0.3876} \\\\\n","\\frac{0.6124 - 1}{0.6124}\n","\\end{bmatrix} = \\begin{bmatrix}\n","1 \\\\\n","-0.6327\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","## 3. Градієнти для другого шару\n","\n","### 3.1 Градієнт по pre-activation ($z^{[2]}$)\n","\\begin{equation*}\n","\\frac{\\partial L}{\\partial z^{[2]}} = \\frac{\\partial L}{\\partial a^{[2]}} \\odot \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} = \\begin{bmatrix}\n","0.3876 \\\\\n","-0.3876\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","### 3.2 Градієнт по вагам $W^{[2]}$\n","\\begin{equation*}\n","\\frac{\\partial L}{\\partial W^{[2]}} = \\frac{\\partial L}{\\partial z^{[2]}} (a^{[1]})^T\n","\\end{equation*}\n","\n","Розпишемо детально:\n","\\begin{equation*}\n","\\frac{\\partial L}{\\partial W^{[2]}} = \\begin{bmatrix}\n","0.3876 \\\\\n","-0.3876\n","\\end{bmatrix} \\begin{bmatrix}\n","0 & 0.2604 & 0.2 & 0.1156\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","\\begin{equation*}\n","\\frac{\\partial L}{\\partial W^{[2]}} = \\begin{bmatrix}\n","0 & 0.1009 & 0.0775 & 0.0448 \\\\\n","0 & -0.1009 & -0.0775 & -0.0448\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","### 3.3 Градієнт по зміщенню $b^{[2]}$\n","\\begin{equation*}\n","\\frac{\\partial L}{\\partial b^{[2]}} = \\frac{\\partial L}{\\partial z^{[2]}} = \\begin{bmatrix}\n","0.3876 \\\\\n","-0.3876\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","## 4. Градієнти для першого шару\n","\n","### 4.1 Градієнт по активації першого шару\n","\\begin{equation*}\n","\\frac{\\partial L}{\\partial a^{[1]}} = (W^{[2]})^T \\frac{\\partial L}{\\partial z^{[2]}}\n","\\end{equation*}\n","\n","\\begin{equation*}\n","\\frac{\\partial L}{\\partial a^{[1]}} = \\begin{bmatrix}\n","0.3003 & -0.1988 \\\\\n","-0.3884 & 0.5115 \\\\\n","0.4 & -0.3 \\\\\n","-0.4997 & 0.2003\n","\\end{bmatrix} \\begin{bmatrix}\n","0.3876 \\\\\n","-0.3876\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","\\begin{equation*}\n","\\frac{\\partial L}{\\partial a^{[1]}} = \\begin{bmatrix}\n","0.1934 \\\\\n","-0.3487 \\\\\n","0.2700 \\\\\n","-0.2700\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","### 4.2 Градієнт ReLU\n","\\begin{equation*}\n","\\frac{\\partial \\text{ReLU}}{\\partial z^{[1]}} = \\begin{cases}\n","1 & \\text{if } z^{[1]} > 0 \\\\\n","0 & \\text{if } z^{[1]} \\leq 0\n","\\end{cases}\n","\\end{equation*}\n","\n","\\begin{equation*}\n","\\text{ReLU}'(z^{[1]}) = \\begin{bmatrix}\n","0 \\\\\n","1 \\\\\n","1 \\\\\n","1\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","### 4.3 Градієнт по pre-activation першого шару\n","\\begin{equation*}\n","\\frac{\\partial L}{\\partial z^{[1]}} = \\frac{\\partial L}{\\partial a^{[1]}} \\odot \\text{ReLU}'(z^{[1]})\n","\\end{equation*}\n","\n","\\begin{equation*}\n","\\frac{\\partial L}{\\partial z^{[1]}} = \\begin{bmatrix}\n","0 \\\\\n","-0.3487 \\\\\n","0.2700 \\\\\n","-0.2700\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","### 4.4 Градієнт по вагам першого шару\n","\\begin{equation*}\n","\\frac{\\partial L}{\\partial W^{[1]}} = \\frac{\\partial L}{\\partial z^{[1]}} x_{\\text{green}}^T\n","\\end{equation*}\n","\n","\\begin{equation*}\n","\\frac{\\partial L}{\\partial W^{[1]}} = \\begin{bmatrix}\n","0 \\\\\n","-0.3487 \\\\\n","0.2700 \\\\\n","-0.2700\n","\\end{bmatrix} \\begin{bmatrix}\n","0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","### 4.5 Градієнт по зміщенню першого шару\n","\\begin{equation*}\n","\\frac{\\partial L}{\\partial b^{[1]}} = \\frac{\\partial L}{\\partial z^{[1]}} = \\begin{bmatrix}\n","0 \\\\\n","-0.3487 \\\\\n","0.2700 \\\\\n","-0.2700\n","\\end{bmatrix}\n","\\end{equation*}\n","\n","## 5. Оновлення параметрів з momentum\n","\n","### 5.1 Оновлення швидкості для ваг\n","\\begin{equation*}\n","v_{W^{[2]}} = \\beta v_{W^{[2]}} + (1-\\beta)\\frac{\\partial L}{\\partial W^{[2]}}\n","\\end{equation*}\n","\n","### 5.2 Оновлення швидкості для зміщення\n","\\begin{equation*}\n","v_{b^{[2]}} = \\beta v_{b^{[2]}} + (1-\\beta)\\frac{\\partial L}{\\partial b^{[2]}}\n","\\end{equation*}\n","\n","### 5.3 Оновлення параметрів\n","\\begin{equation*}\n","W^{[2]} = W^{[2]} - \\alpha v_{W^{[2]}}\n","\\end{equation*}\n","\n","\\begin{equation*}\n","b^{[2]} = b^{[2]} - \\alpha v_{b^{[2]}}\n","\\end{equation*}\n","\n","де $\\alpha$ - швидкість навчання, $\\beta$ - параметр momentum.\n"],"metadata":{"id":"GKBR2-TKg7kk"}}]}