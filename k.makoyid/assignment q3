from cs231n.classifiers.softmax import softmax_loss_naive
from cs231n.classifiers.linear_classifier import LinearClassifier
import time
import numpy as np
import matplotlib.pyplot as plt

W = np.random.randn(X_train.shape[1], 10) * np.sqrt(2.0 / X_train.shape[1])
loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)

# As a rough sanity check, our loss should be something close to -log(0.1).
print('loss: %f' % loss)
print('sanity check: %f' % (-np.log(0.1)))

# Complete the implementation of softmax_loss_naive and implement a (naive)
# version of the gradient that uses nested loops.
def softmax_loss_naive(W, X, y, reg):
    loss = 0.0
    dW = np.zeros_like(W)
    num_classes = W.shape[1]
    num_train = X.shape[0]
    
    for i in range(num_train):
        scores = X[i].dot(W)
        scores -= np.max(scores) 
        exp_scores = np.exp(scores)
        probs = exp_scores / np.sum(exp_scores)
        
        loss -= np.log(probs[y[i]])
        
        for j in range(num_classes):
            dW[:, j] += X[i] * probs[j]
            if j == y[i]:
                dW[:, j] -= X[i]
    
    loss /= num_train
    dW /= num_train
    
    loss += 0.5 * reg * np.sum(W * W)
    dW += reg * W
    
    return loss, dW

loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)

# As we did for the SVM, use numeric gradient checking as a debugging tool.
# The numeric gradient should be close to the analytic gradient.
from cs231n.gradient_check import grad_check_sparse
f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]
grad_numerical = grad_check_sparse(f, W, grad, 10)

# similar to SVM case, do another gradient check with regularization
loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)
f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]
grad_numerical = grad_check_sparse(f, W, grad, 10)

# Now that we have a naive implementation of the softmax loss function and its gradient,
# implement a vectorized version in softmax_loss_vectorized.
# The two versions should compute the same results, but the vectorized version should be
# much faster.
def softmax_loss_vectorized(W, X, y, reg):
    num_classes = W.shape[1]
    num_train = X.shape[0]
    
    scores = X.dot(W)
    scores -= np.max(scores, axis=1, keepdims=True)
    exp_scores = np.exp(scores)
    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
    
    correct_logprobs = -np.log(probs[range(num_train), y])
    loss = np.sum(correct_logprobs) / num_train + 0.5 * reg * np.sum(W * W)
    
    dscores = probs
    dscores[range(num_train), y] -= 1
    dscores /= num_train
    
    dW = X.T.dot(dscores)
    dW += reg * W
    
    return loss, dW

tic = time.time()
loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)
toc = time.time()
print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))

tic = time.time()
loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)
toc = time.time()
print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))

grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')
print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))
print('Gradient difference: %f' % grad_difference)

class Softmax(LinearClassifier):
    def __init__(self):
        self.W = None

    def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,
              batch_size=200, verbose=False):
        num_train, dim = X.shape
        num_classes = np.max(y) + 1  

        if self.W is None:

            self.W = 0.001 * np.random.randn(dim, num_classes)

        loss_history = []

        for it in range(num_iters):
            X_batch = None
            y_batch = None

            indices = np.random.choice(num_train, batch_size, replace=False)
            X_batch = X[indices]
            y_batch = y[indices]

            loss, grad = self.loss(X_batch, y_batch, reg)
            loss_history.append(loss)

            self.W -= learning_rate * grad

            if verbose and it % 100 == 0:
                print('iteration %d / %d: loss %f' % (it, num_iters, loss))

            if it > 5 and np.abs(loss_history[-1] - loss_history[-2]) < 1e-5:
                print('Early stopping at iteration', it)
                break

        return loss_history

    def loss(self, X_batch, y_batch, reg):
        return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)

    def predict(self, X):
        y_pred = np.zeros(X.shape[0])
        scores = X.dot(self.W)
        y_pred = np.argmax(scores, axis=1)
        return y_pred

learning_rates = [1e-7, 5e-7, 1e-6, 2e-6, 5e-6, 1e-5]
regularization_strengths = [500, 750, 1000, 1250, 1500, 2000]

results = {}
best_val = -1
best_softmax = None
best_params = None

num_iters = 5000
batch_size = 200

for lr in learning_rates:
    for reg in regularization_strengths:
        softmax = Softmax()
        loss_history = softmax.train(X_train, y_train, learning_rate=lr, reg=reg,
                                     num_iters=num_iters, batch_size=batch_size, 
                                     verbose=False)
        
        y_train_pred = softmax.predict(X_train)
        train_accuracy = np.mean(y_train == y_train_pred)
        y_val_pred = softmax.predict(X_val)
        val_accuracy = np.mean(y_val == y_val_pred)
        
        results[(lr, reg)] = (train_accuracy, val_accuracy)
        
        if val_accuracy > best_val:
            best_val = val_accuracy
            best_softmax = softmax
            best_params = (lr, reg)
        
        print('lr %e reg %e train accuracy: %f val accuracy: %f' % (
              lr, reg, train_accuracy, val_accuracy))

plt.plot(loss_history)
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.show()

for lr, reg in sorted(results):
    train_accuracy, val_accuracy = results[(lr, reg)]
    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (
                lr, reg, train_accuracy, val_accuracy))
    
print('best validation accuracy achieved during cross-validation: %f' % best_val)
print('best params:', best_params)

y_test_pred = best_softmax.predict(X_test)
test_accuracy = np.mean(y_test == y_test_pred)
print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))

--------------------------------------------------------------------------------------------------------------------
result:
loss: 0.000000
sanity check: 2.302585
numerical: 1.864366 analytic: 1.864366, relative error: 1.091112e-10
numerical: -0.289799 analytic: -0.289799, relative error: 4.847023e-09
numerical: -1.502692 analytic: -1.502692, relative error: 3.675042e-10
numerical: -0.924676 analytic: -0.924676, relative error: 1.169383e-09
numerical: 0.639126 analytic: 0.639126, relative error: 3.573420e-09
numerical: -0.787469 analytic: -0.787469, relative error: 2.795682e-09
numerical: -0.281263 analytic: -0.281263, relative error: 1.018733e-08
numerical: 1.272888 analytic: 1.272888, relative error: 1.441340e-09
numerical: -1.032656 analytic: -1.032656, relative error: 1.917009e-09
numerical: -9.267289 analytic: -9.267289, relative error: 7.976661e-11
numerical: -2.609780 analytic: -2.609780, relative error: 1.133844e-09
numerical: 1.356714 analytic: 1.356714, relative error: 1.388851e-09
numerical: -2.282893 analytic: -2.282893, relative error: 1.845666e-09
numerical: -1.507739 analytic: -1.507739, relative error: 1.866523e-09
numerical: -6.128260 analytic: -6.128260, relative error: 3.049063e-11
numerical: -0.266354 analytic: -0.266354, relative error: 2.590916e-09
numerical: 3.159842 analytic: 3.159842, relative error: 1.726108e-10
numerical: -5.332924 analytic: -5.332924, relative error: 2.805043e-10
numerical: 1.646354 analytic: 1.646354, relative error: 4.965013e-10
numerical: -2.283930 analytic: -2.283930, relative error: 8.512317e-11
naive loss: 1.356708e+02 computed in 0.101964s
vectorized loss: 1.356708e+02 computed in 0.010955s
Loss difference: 0.000000
Gradient difference: 0.000000
lr 1.000000e-07 reg 5.000000e+02 train accuracy: 0.320857 val accuracy: 0.332000
lr 1.000000e-07 reg 7.500000e+02 train accuracy: 0.322143 val accuracy: 0.321000
lr 1.000000e-07 reg 1.000000e+03 train accuracy: 0.334755 val accuracy: 0.320000
lr 1.000000e-07 reg 1.250000e+03 train accuracy: 0.341653 val accuracy: 0.346000
Early stopping at iteration 3836
lr 1.000000e-07 reg 1.500000e+03 train accuracy: 0.330122 val accuracy: 0.326000
lr 1.000000e-07 reg 2.000000e+03 train accuracy: 0.359000 val accuracy: 0.358000
lr 5.000000e-07 reg 5.000000e+02 train accuracy: 0.413245 val accuracy: 0.405000
Early stopping at iteration 4372
lr 5.000000e-07 reg 7.500000e+02 train accuracy: 0.415265 val accuracy: 0.411000
lr 5.000000e-07 reg 1.000000e+03 train accuracy: 0.415531 val accuracy: 0.414000
Early stopping at iteration 207
lr 5.000000e-07 reg 1.250000e+03 train accuracy: 0.239959 val accuracy: 0.232000
lr 5.000000e-07 reg 1.500000e+03 train accuracy: 0.407837 val accuracy: 0.397000
lr 5.000000e-07 reg 2.000000e+03 train accuracy: 0.403367 val accuracy: 0.412000
lr 1.000000e-06 reg 5.000000e+02 train accuracy: 0.423776 val accuracy: 0.414000
Early stopping at iteration 1836
lr 1.000000e-06 reg 7.500000e+02 train accuracy: 0.408020 val accuracy: 0.394000
lr 1.000000e-06 reg 1.000000e+03 train accuracy: 0.412857 val accuracy: 0.418000
lr 1.000000e-06 reg 1.250000e+03 train accuracy: 0.409367 val accuracy: 0.406000
lr 1.000000e-06 reg 1.500000e+03 train accuracy: 0.407939 val accuracy: 0.412000
Early stopping at iteration 3420
lr 1.000000e-06 reg 2.000000e+03 train accuracy: 0.398612 val accuracy: 0.392000
lr 2.000000e-06 reg 5.000000e+02 train accuracy: 0.411306 val accuracy: 0.416000
Early stopping at iteration 3220
lr 2.000000e-06 reg 7.500000e+02 train accuracy: 0.409041 val accuracy: 0.403000
lr 2.000000e-06 reg 1.000000e+03 train accuracy: 0.404755 val accuracy: 0.402000
lr 2.000000e-06 reg 1.250000e+03 train accuracy: 0.393898 val accuracy: 0.395000
lr 2.000000e-06 reg 1.500000e+03 train accuracy: 0.399796 val accuracy: 0.398000
lr 2.000000e-06 reg 2.000000e+03 train accuracy: 0.395204 val accuracy: 0.401000
lr 5.000000e-06 reg 5.000000e+02 train accuracy: 0.370367 val accuracy: 0.355000
lr 5.000000e-06 reg 7.500000e+02 train accuracy: 0.346898 val accuracy: 0.364000
lr 5.000000e-06 reg 1.000000e+03 train accuracy: 0.314122 val accuracy: 0.324000
lr 5.000000e-06 reg 1.250000e+03 train accuracy: 0.369714 val accuracy: 0.372000
lr 5.000000e-06 reg 1.500000e+03 train accuracy: 0.356286 val accuracy: 0.345000
Early stopping at iteration 955
lr 5.000000e-06 reg 2.000000e+03 train accuracy: 0.333633 val accuracy: 0.346000
lr 1.000000e-05 reg 5.000000e+02 train accuracy: 0.253633 val accuracy: 0.261000
lr 1.000000e-05 reg 7.500000e+02 train accuracy: 0.293592 val accuracy: 0.297000
lr 1.000000e-05 reg 1.000000e+03 train accuracy: 0.297327 val accuracy: 0.294000
lr 1.000000e-05 reg 1.250000e+03 train accuracy: 0.255898 val accuracy: 0.257000
lr 1.000000e-05 reg 1.500000e+03 train accuracy: 0.299408 val accuracy: 0.308000
lr 1.000000e-05 reg 2.000000e+03 train accuracy: 0.255857 val accuracy: 0.277000

lr 1.000000e-07 reg 5.000000e+02 train accuracy: 0.320857 val accuracy: 0.332000
lr 1.000000e-07 reg 7.500000e+02 train accuracy: 0.322143 val accuracy: 0.321000
lr 1.000000e-07 reg 1.000000e+03 train accuracy: 0.334755 val accuracy: 0.320000
lr 1.000000e-07 reg 1.250000e+03 train accuracy: 0.341653 val accuracy: 0.346000
lr 1.000000e-07 reg 1.500000e+03 train accuracy: 0.330122 val accuracy: 0.326000
lr 1.000000e-07 reg 2.000000e+03 train accuracy: 0.359000 val accuracy: 0.358000
lr 5.000000e-07 reg 5.000000e+02 train accuracy: 0.413245 val accuracy: 0.405000
lr 5.000000e-07 reg 7.500000e+02 train accuracy: 0.415265 val accuracy: 0.411000
lr 5.000000e-07 reg 1.000000e+03 train accuracy: 0.415531 val accuracy: 0.414000
lr 5.000000e-07 reg 1.250000e+03 train accuracy: 0.239959 val accuracy: 0.232000
lr 5.000000e-07 reg 1.500000e+03 train accuracy: 0.407837 val accuracy: 0.397000
lr 5.000000e-07 reg 2.000000e+03 train accuracy: 0.403367 val accuracy: 0.412000
lr 1.000000e-06 reg 5.000000e+02 train accuracy: 0.423776 val accuracy: 0.414000
lr 1.000000e-06 reg 7.500000e+02 train accuracy: 0.408020 val accuracy: 0.394000
lr 1.000000e-06 reg 1.000000e+03 train accuracy: 0.412857 val accuracy: 0.418000
lr 1.000000e-06 reg 1.250000e+03 train accuracy: 0.409367 val accuracy: 0.406000
lr 1.000000e-06 reg 1.500000e+03 train accuracy: 0.407939 val accuracy: 0.412000
lr 1.000000e-06 reg 2.000000e+03 train accuracy: 0.398612 val accuracy: 0.392000
lr 2.000000e-06 reg 5.000000e+02 train accuracy: 0.411306 val accuracy: 0.416000
lr 2.000000e-06 reg 7.500000e+02 train accuracy: 0.409041 val accuracy: 0.403000
lr 2.000000e-06 reg 1.000000e+03 train accuracy: 0.404755 val accuracy: 0.402000
lr 2.000000e-06 reg 1.250000e+03 train accuracy: 0.393898 val accuracy: 0.395000
lr 2.000000e-06 reg 1.500000e+03 train accuracy: 0.399796 val accuracy: 0.398000
lr 2.000000e-06 reg 2.000000e+03 train accuracy: 0.395204 val accuracy: 0.401000
lr 5.000000e-06 reg 5.000000e+02 train accuracy: 0.370367 val accuracy: 0.355000
lr 5.000000e-06 reg 7.500000e+02 train accuracy: 0.346898 val accuracy: 0.364000
lr 5.000000e-06 reg 1.000000e+03 train accuracy: 0.314122 val accuracy: 0.324000
lr 5.000000e-06 reg 1.250000e+03 train accuracy: 0.369714 val accuracy: 0.372000
lr 5.000000e-06 reg 1.500000e+03 train accuracy: 0.356286 val accuracy: 0.345000
lr 5.000000e-06 reg 2.000000e+03 train accuracy: 0.333633 val accuracy: 0.346000
lr 1.000000e-05 reg 5.000000e+02 train accuracy: 0.253633 val accuracy: 0.261000
lr 1.000000e-05 reg 7.500000e+02 train accuracy: 0.293592 val accuracy: 0.297000
lr 1.000000e-05 reg 1.000000e+03 train accuracy: 0.297327 val accuracy: 0.294000
lr 1.000000e-05 reg 1.250000e+03 train accuracy: 0.255898 val accuracy: 0.257000
lr 1.000000e-05 reg 1.500000e+03 train accuracy: 0.299408 val accuracy: 0.308000
lr 1.000000e-05 reg 2.000000e+03 train accuracy: 0.255857 val accuracy: 0.277000
best validation accuracy achieved during cross-validation: 0.418000
best params: (1e-06, 1000)
softmax on raw pixels final test set accuracy: 0.388000
